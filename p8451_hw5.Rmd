---
title: "Machine Learning for Epi: Assignment 5"
output:
  html_document: default
  word_document: default
date: "2023-02-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = F,
                      message = F)

library(tidyverse)
library(caret)
library(klaR)
library(glmnet)
```

# Predicting Current Alcohol Consumption from Behavioral Scores

## Description of Data

The data we will be using are from the online survey related to drug and alcohol use and personality traits. These data were collected as part of an online survey related to drug and alcohol use and personality traits from the UCI Machine Learning Repository. *We will be using this dataset to try to identify the most important behavioral predictors of alcohol consumption.* We have restricted the dataset to 7 features and an outcome which distinguishes those who reported current alcohol use (defined as alcohol use in the past month or more frequently) vs no current use. 

### Step 1: Load data and prepare for analysis

The code chunk below loads the Alcohol Consumption survey data and strips the id variable, omits missing observations, and converts the outcome variable, `alc_consumption` to a factor variable. 

```{r load_data}
alcohol_use = readr::read_csv("./alcohol_use.csv") %>% 
  mutate(alc_consumption = as.factor(alc_consumption), 
         alc_consumption = fct_relevel(alc_consumption, c("NotCurrentUse", "CurrentUse"))) %>% 
  dplyr::select(-`...1`) %>% 
  drop_na()  
  
summary(alcohol_use) %>% knitr::kable(digits = 2)
```

Our resulting variables include 7 behavioral/personality scores, which are numeric, and our binary outcome variable, `alc_consumption`. Based on the summary, we can see that the distribution is similar across the numeric variables, and appears to be already scaled and centered. Therefore, will skip the centering and scaling steps in pre-processing.

### Step 2: Partition the data 

The code chunk below partitions the data into training and testing sets, using a 70/30 split. 

```{r partition_data}
set.seed(123)

#Creating balanced partitions in the data
train_index = createDataPartition(alcohol_use$alc_consumption, p = 0.7, list = FALSE)

alc_train = alcohol_use[train_index,]
alc_test = alcohol_use[-train_index,]

#Check distribution of the outcome between train and test data
summary(alc_train$alc_consumption) 
summary(alc_test$alc_consumption)
```

We can see that there are similar distributions of the variable `alc_consumption`, with approximately 53% of cases of alcohol use across both the training and testing sets, indicating that the data were successfully partitioned.

### Step 4: Construct logistic regression models to predict healthy days

We will fit 3 regularized and traditional logistic models to predict current alcohol consumption. (feature name: `alc_consumption`).

- Model 1 (`mod_elastic`): An elastic net model based on all features, using cross-validation to choose alpha and lambda.

- Model 2 (`mod_baseline`): An traditional logistic model based on all features, serving as our 'baseline model'.

- Model 3 (`mod_lasso`): A LASSO logistic model based on all features.

These models will be trained on the training dataset using 10-fold cross validation. 

```{r mod_elastic}
set.seed(123)

mod_elastic = train(alc_consumption ~ ., data = alc_train, method = "glmnet", 
                 trControl = trainControl("cv", number = 10), 
                 tuneLength = 10)
#Print the values of alpha and lambda that gave best prediction
mod_elastic$bestTune

# Model coefficients
coef(mod_elastic$finalModel, mod_elastic$bestTune$lambda)
```

```{r mod_logistic}
set.seed(123)

mod_baseline = train(alc_consumption ~ ., data = alc_train, method = "glm", 
                 trControl = trainControl("cv", number = 10))

coef(mod_baseline$finalModel)

```

```{r mod_LASSO}
set.seed(123)

#Create grid to search lambda
lambda = 10^seq(-3, 3, length = 100)

#Fit model with tuneGrid
mod_lasso = train(alc_consumption ~ ., data = alc_train, method = "glmnet", 
                  trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda))

#Print the values of alpha and lambda that gave best prediction
mod_lasso$bestTune

# Model coefficients
coef(mod_lasso$finalModel, mod_lasso$bestTune$lambda)
```


Results of the cross-validated training shows the model with all variables, `lm_all`, has a slightly lower RMSE compared to the model with only health habits. However, we want to confirm this by running each model on the testing data.

### Step 5: Model Evaluation

Next, to determine the preferred prediction model, we will apply both models to the test data, and compare their performance based on the RMSE, which is an evaluation metric used for linear regression models.

```{r test_EN}
## ELASTIC MODEL
# Make predictions in test set
en_pred = mod_elastic %>% predict(alc_test)
alc_test = alc_test %>% mutate(en_pred = as.factor(en_pred))

# Model prediction performance
elastic_perf = postResample(en_pred, alc_test$alc_consumption) %>% as_tibble_row()

ggplot(mod_elastic)
confusionMatrix(data = alc_test$en_pred, reference = alc_test$alc_consumption, positive = "CurrentUse")
```

```{r test_baseline}
## LOGISTIC REG MODEL
# Make predictions in test set
logmod_pred = mod_baseline %>% predict(alc_test)
alc_test = alc_test %>% mutate(logmod_pred = as.factor(logmod_pred))

# Model prediction performance
baseline_perf = postResample(logmod_pred, alc_test$alc_consumption) %>% as_tibble_row()

confusionMatrix(data = alc_test$logmod_pred, reference = alc_test$alc_consumption, positive = "CurrentUse")
```

```{r test_LASSO}
## ELASTIC MODEL
# Make predictions in test set
lasso_pred = mod_lasso %>% predict(alc_test)
alc_test = alc_test %>% mutate(lasso_pred = as.factor(lasso_pred))

# Model prediction performance
LASSO_perf = postResample(lasso_pred, alc_test$alc_consumption) %>% as_tibble_row()

confusionMatrix(data = alc_test$lasso_pred, reference = alc_test$alc_consumption, positive = "CurrentUse")

rbind(elastic_perf, baseline_perf, LASSO_perf) %>% 
  mutate(Model = c("Elastic Net", "Baseline", "LASSO")) %>% 
  relocate(Model, .before = Accuracy) %>% knitr::kable()
```

The table shows that the `lm_all` model performs better on the test data, with an RMSE of 7.172, compared to the `lm_habits` model's RMSE of 7.413. Therefore, if I were only interested in prediction performance, I would select the linear model with all features over the linear model with health habit features to predict the number of healthy days in a month.


### Research Applications

One setting where the `lm_all` model would be useful would be in a program evaluation setting. For instance, in a health program that promotes increased physical activity, when enrolling new subjects, we could predict the number of healthy days they are predicted to have in a month at baseline, and then compare the number of observed healthy days after program participation. This provides evidence for evaluating the success of the physical activity program in improving the number of healthy days a person may have.

